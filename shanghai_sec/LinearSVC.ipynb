{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv',encoding='utf-8')\n",
    "train= pd.read_csv('train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.iloc[:,0]\n",
    "train_y = train.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl = LabelEncoder()\n",
    "train_y = lbl.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先'/'再'-'再'.'再'com'\n",
    "def getTokens(input):\n",
    "    tokensBySlash = str(input.encode('utf-8')).split('/')\t#get tokens after splitting by slash\n",
    "    allTokens = []\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\t#get tokens after splitting by dash\n",
    "        tokensByDot = []\n",
    "        for j in range(0,len(tokens)):\n",
    "            tempTokens = str(tokens[j]).split('.')\t#get tokens after splitting by dot\n",
    "            tokensByDot = tokensByDot + tempTokens\n",
    "        allTokens = allTokens + tokens + tokensByDot\n",
    "    allTokens = list(allTokens)#-remove_set--#remove redundant tokens\n",
    "    if 'com' in allTokens:\n",
    "        allTokens.remove('com')\t#removing .com since it occurs a lot of times and it should not be included in our features\n",
    "    return allTokens\n",
    "\n",
    "def preprocessing_data_2(list_data):\n",
    "    new_data = ''\n",
    "    for i in list_data:\n",
    "        words = nltk.tokenize.TweetTokenizer(strip_handles=False, reduce_len = True).tokenize(i)\n",
    "        temp = ' '.join(words)\n",
    "        new_data = new_data + ' '+temp\n",
    "    return new_data\n",
    "\n",
    "def pipeline_x(data):\n",
    "    data = getTokens(data)\n",
    "    data = preprocessing_data_2(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = train_x.apply(lambda x:pipeline_x(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "HashingVectorizer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "\n",
    "print('TfidfVectorizer')\n",
    "tf = TfidfVectorizer(ngram_range=(1,2),analyzer='char')\n",
    "discuss_tf = tf.fit_transform(pre_data)\n",
    " \n",
    "print('HashingVectorizer')\n",
    "ha = HashingVectorizer(ngram_range=(1,1),lowercase=False)\n",
    "discuss_ha = ha.fit_transform(pre_data)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "data = hstack((discuss_tf,discuss_ha)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum 420464\n",
      "indexs 378418\n"
     ]
    }
   ],
   "source": [
    "#对数据进行分割\n",
    "#先打乱\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = train_y[indices]\n",
    "\n",
    "train_sample = data.shape[0]  - (data.shape[0] // 10)\n",
    "print('sum',data.shape[0])\n",
    "print('indexs',train_sample)\n",
    "\n",
    "x_train = data[:train_sample]\n",
    "y_train = labels[:train_sample]\n",
    "x_val = data[train_sample:]\n",
    "y_val = labels[train_sample:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378418, 1050478)\n",
      "(42046, 1050478)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer,TfidfVectorizer\n",
    " \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import f1_score,accuracy_score,classification_report\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import pickle as pk\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "kf = StratifiedKFold(n_splits=N, random_state=2018).split(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oofp = np.zeros_like(y_train, dtype='float32')\n",
    "y_train_oofp1 = np.zeros_like(y_train, dtype='float32')\n",
    " \n",
    "y_test_oofp = np.zeros((test.shape[0], N))\n",
    "y_test_oofp_1 = np.zeros((test.shape[0], N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(C=0.6, loss='hinge', tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_avg_f1(y_true, y_pred):\n",
    "    return metrics.f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_value_f1:0.981497\n",
      "sentiment_value_f1:0.981639\n",
      "sentiment_value_f1:0.981853\n",
      "sentiment_value_f1:0.981140\n",
      "sentiment_value_f1:0.982210\n",
      "sentiment_value_f1:0.981925\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 378428 is out of bounds for axis 0 with size 378418",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c98ab3457445>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0my_train_oofp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_fold\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sentiment_value_f1:%f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmicro_avg_f1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmicro_avg_f1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 378428 is out of bounds for axis 0 with size 378418"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "vcc = 0\n",
    "for i ,(train_fold,test_fold) in enumerate(kf):\n",
    "    X_train, X_validate, label_train, label_validate = data[train_fold, :], data[test_fold, :], labels[train_fold], labels[test_fold]\n",
    "    clf.fit(X_train, label_train)\n",
    "    \n",
    "    val_ = clf.predict(X_validate)\n",
    "    y_train_oofp[test_fold] = val_\n",
    "    print('sentiment_value_f1:%f' % micro_avg_f1(label_validate, val_))\n",
    "    acc += micro_avg_f1(label_validate, val_)\n",
    "\n",
    " \n",
    "print(acc/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
